{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9pWsPAt0dCh",
    "tags": []
   },
   "source": [
    "# Assignment 3 Part 2 - Find complex answers to medical questions\n",
    "\n",
    "*Submission deadline: Friday 1 November 2024, 11:55pm.*\n",
    "\n",
    "*Assessment marks: 20 marks (20% of the total unit assessment)*\n",
    "\n",
    "Unless a Special Consideration request has been submitted and approved, a 5% penalty (of the total possible mark of the task) will be applied for each day a written report or presentation assessment is not submitted, up until the 7th day (including weekends). After the 7th day, a grade of ‘0’ will be awarded even if the assessment is submitted. For example, if the assignment is worth 8 marks (of the entire unit) and your submission is late by 19 hours (or 23 hours 59 minutes 59 seconds), 0.4 marks (5% of 8 marks) will be deducted. If your submission is late by 24 hours (or 47 hours 59 minutes 59 seconds), 0.8 marks (10% of 8 marks) will be deducted, and so on. The submission time for all uploaded assessments is 11:55 pm. A 1-hour grace period will be provided to students who experience a technical concern. For any late submission of time-sensitive tasks, such as scheduled tests/exams, performance assessments/presentations, and/or scheduled practical assessments/labs, please apply for [Special Consideration](https://students.mq.edu.au/study/assessment-exams/special-consideration).\n",
    "\n",
    "Note that the work submitted should be your own work. For rules of using of AI tools, refer to \"Using Generative AI Tools\" on iLearn.\n",
    "\n",
    "\n",
    "# A note on the use of AI generators\n",
    "In this assignment, we view AI code generators such as copilot, CodeGPT, etc as tools that can help you write code quickly. You are allowed to use these tools, but with some conditions. To understand what you can and what you cannot do, please visit these information pages provided by Macquarie University: \n",
    "\n",
    "Artificial Intelligence Tools and Academic Integrity in FSE - https://bit.ly/3uxgQP4\n",
    "\n",
    "If you choose to use these tools, make the following explicit in your submitted file as comments starting with \"Use of AI generators in this assignment\" explain:\n",
    "\n",
    "* What part of your code is based on the output of such tools,\n",
    "* What tools you used,\n",
    "* What prompts you used to generate the code or text, and\n",
    "* What modifications you made on the generated code or text.\n",
    "\n",
    "\n",
    "This will help us assess your work fairly. If we observe that you have used an AI generator and you do not give the above information, you may face disciplinary action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives of this assignment\n",
    "\n",
    "In assignment 3 you will work on a general answer selection task. Given a question and a list of sentences, the final goal is to predict which of these sentences from the list can be used as part of the answer to the question. Assignment 3 is divided into two parts. Part 1 will help you get familiar with the data, and Part 2 requires you to implement deep neural networks.\n",
    "\n",
    "The data is in the file `train.csv`, which is provided in both GitHub repository and in iLearn. Each row of the file consists of a question ('qtext' column), an answer ('atext' column), and a label ('label' column) that indicates whether the  answer is correctly related to the question (1) or not (0).\n",
    "\n",
    "The following code uses pandas to store the file `train.csv` in a data frame and shows the first few rows of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the left-most index is not part of the data, it is added by ipynb automatically for easy reading. You can also browse the data using Microsoft Excel or similar software."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's get started.\n",
    "\n",
    "Use the provided files `train.csv`, `val.csv`, and `test.csv` in the data.zip file for all the tasks below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction\n",
    "* You are required to finish the two tasks below.\n",
    "* You need to write code in this ipynb file.\n",
    "* Your ipynb file needs to include the running outputs of your final code. \n",
    "* **You need to submit this ipynb file, containing your code and outputs, to iLearn.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment\n",
    "\n",
    "1. We mark based on the correctness of your code, outputs, and coding style. \n",
    "2. We assign 2 marks (1 mark each Task) for good coding style, including but not limited to clean codes, self-explained variable names, good comments that help understand the code, etc.\n",
    "3. We assign 2 marks (1 mark each Task) for correctly feeding data into your model, and correctly training and testing of your models.\n",
    "4. 2 marks will be deducted for the task that does not have outputs or its outputs are incorrect.\n",
    "4. For the remaining detailed marks, please refer to each specific task below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VTTgRnN0dC4"
   },
   "source": [
    "# Task 1 (8 marks): Simple Siamese NN - Contrastive Loss\n",
    "\n",
    "Implement a simple TensorFlow-Keras neural model that meets the following requirements:\n",
    "\n",
    "1. (0.5 marks) An input layer that will accept the tf.idf of paired data. The input of the Siamese network is a pair of data, i.e., (qtext, atext). \n",
    "2. (2 marks) Use two hidden layers and a ReLU activation function. You need to determine the size of the hidden layers in {64, 128, 256} using val data, assuming these two layers use the same hidden size.\n",
    "3. (0.5 marks) Use Euclidean-distance-based contrastive loss to train the model.\n",
    "4. (0.5 marks) Use Sigmoid function for classification.\n",
    "5. (1 mark) Calculate prediction accuracy.\n",
    "6. (1.5 marks) Give an example of failure case, and explain the possible reason and discuss potential solution. \n",
    "7. (1 mark) Good coding style as explained in the above Assessment Section.\n",
    "8. (1 mark) Correctly feeding data into your model, and correctly training and testing of your models.\n",
    "\n",
    "Use the test data to report the final accuracy of your best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Right before submission, when I was tidying up an error occured and now I have trouble importing libraries...\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Please note you might encounter the same issue when running all cells.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rankdata\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scipy/stats/__init__.py:608\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    603\u001b[0m \n\u001b[1;32m    604\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_warnings_errors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[1;32m    607\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[0;32m--> 608\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_py\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_variation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m variation\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scipy/stats/_stats_py.py:39\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NumpyVersion\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m suppress_warnings\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cdist\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mndimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _measurements\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (check_random_state, MapWrapper, _get_nan,\n\u001b[1;32m     42\u001b[0m                               rng_integers, _rename_parameter, _contains_nan)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scipy/spatial/__init__.py:110\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m=============================================================\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mSpatial algorithms and data structures (:mod:`scipy.spatial`)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m   QhullError\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_kdtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ckdtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_qhull\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scipy/spatial/_kdtree.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright Anne M. Archibald 2008\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Released under the scipy license\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ckdtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cKDTree, cKDTreeNode\n\u001b[1;32m      6\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminkowski_distance_p\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminkowski_distance\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance_matrix\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRectangle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKDTree\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mminkowski_distance_p\u001b[39m(x, y, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n",
      "File \u001b[0;32m_ckdtree.pyx:1\u001b[0m, in \u001b[0;36minit scipy.spatial._ckdtree\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "# Right before submission, when I was tidying up an error occured and now I have trouble importing libraries...\n",
    "# Please note you might encounter the same issue when running all cells.\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import rankdata\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn \n",
    "import os\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras import layers, modelsY\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "dataset = pd.read_csv(\"train.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uY6sDbUn0dC6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "val_df = pd.read_csv('val.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "print(train_df.shape)\n",
    "print(val_df.shape)\n",
    "print(test_df.shape)\n",
    "\n",
    "train_df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF vectorizer\n",
    "# Changing the number of features used can increase the accuracy.\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Combine all texts because we want the same number of features used for q and a\n",
    "all_train_texts = list(train_df['qtext']) + list(train_df['atext'])\n",
    "\n",
    "# Fit vectorizer on training data only\n",
    "vectorizer.fit(all_train_texts)\n",
    "\n",
    "# Transform each dataset\n",
    "def transform_dataset(df):\n",
    "    q_features = vectorizer.transform(df['qtext']).toarray()\n",
    "    a_features = vectorizer.transform(df['atext']).toarray()\n",
    "    labels = df['label'].values\n",
    "    return q_features, a_features, labels\n",
    "\n",
    "# Transform all datasets\n",
    "train_features = transform_dataset(train_df)\n",
    "val_features = transform_dataset(val_df)\n",
    "test_features = transform_dataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def siamese_model(input_dim, hidden_size):\n",
    "\n",
    "    # Input layers\n",
    "    input_q = layers.Input(shape=(input_dim,))\n",
    "    input_a = layers.Input(shape=(input_dim,))\n",
    "    \n",
    "    # Shared encoder layers\n",
    "    def create_encoder():\n",
    "        return tf.keras.Sequential([\n",
    "            layers.Dense(hidden_size, activation='relu', kernel_initializer='he_normal'),\n",
    "            layers.Dense(hidden_size, activation='relu', kernel_initializer='he_normal')\n",
    "        ])\n",
    "    \n",
    "    encoder = create_encoder()\n",
    "    \n",
    "    # Encode both inputs\n",
    "    tower_q = encoder(input_q)\n",
    "    tower_a = encoder(input_a)\n",
    "    \n",
    "    # Euclidean distance\n",
    "    distance = layers.Lambda(\n",
    "        lambda tensors: tf.sqrt(tf.reduce_sum(tf.square(tensors[0] - tensors[1]), axis=-1, keepdims=True))\n",
    "    )([tower_q, tower_a])\n",
    "    \n",
    "    # Output layer with sigmoid activation\n",
    "    output = layers.Dense(1, activation='sigmoid')(distance)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=[input_q, input_a], outputs=output)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hidden_size(train_data, val_data):\n",
    "    \"\"\"\n",
    "    Find the best hidden layer size using validation data\n",
    "    \"\"\"\n",
    "    hidden_sizes = [64, 128, 256]\n",
    "    best_val_acc = 0\n",
    "    best_size = None\n",
    "    best_model = None\n",
    "    \n",
    "    # Unpack the training and validation data\n",
    "    q_train, a_train, y_train = train_data\n",
    "    q_val, a_val, y_val = val_data\n",
    "    \n",
    "    # Get input dimension from data\n",
    "    input_dim = q_train.shape[1]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for hidden_size in hidden_sizes:\n",
    "        print(f\"\\nTrying hidden size: {hidden_size}\")\n",
    "        \n",
    "        # Create and train model\n",
    "        model = siamese_model(input_dim, hidden_size)\n",
    "        \n",
    "        history = model.fit(\n",
    "            [q_train, a_train],\n",
    "            y_train,\n",
    "            validation_data=([q_val, a_val], y_val),\n",
    "            epochs=10,\n",
    "            batch_size=32,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Get validation accuracy\n",
    "        val_loss, val_acc = model.evaluate([q_val, a_val], y_val, verbose=0)\n",
    "        results.append((hidden_size, val_acc))\n",
    "        \n",
    "        print(f\"Validation accuracy with hidden size {hidden_size}: {val_acc:.4f}\")\n",
    "        \n",
    "        # Update best model if necessary\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_size = hidden_size\n",
    "            best_model = model\n",
    "    \n",
    "    print(\"\\nHidden size tuning results:\")\n",
    "    for size, acc in results:\n",
    "        print(f\"Hidden size {size}: {acc:.4f}\")\n",
    "    print(f\"\\nBest hidden size: {best_size} (validation accuracy: {best_val_acc:.4f})\")\n",
    "    \n",
    "    return best_model, best_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the tuning function\n",
    "# val_accuracy came back with nan value so many times, and now fixed.\n",
    "best_model, best_size = tune_hidden_size(train_features, val_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best hidden size: 128 (validation accuracy: 0.5498)\n",
    "Lets build Siameas network with 2 hidden_layers with size 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_dim = q_train.shape[1]\n",
    "\n",
    "siamea = siamese_model(input_dim, 256)\n",
    "\n",
    "siamea.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transform validation dataset\n",
    "q_val, a_val, y_val = transform_dataset(val_df)\n",
    "q_train, a_train, y_train = transform_dataset(train_df)\n",
    "\n",
    "# Confirming the size so as to verify an issue earlier happened.\n",
    "print(\"q_train:\", q_train.shape)\n",
    "print(\"a_train:\", a_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"q_val:\", q_val.shape)\n",
    "print(\"a_val:\", a_val.shape)\n",
    "print(\"y_val:\", y_val.shape)\n",
    "\n",
    "\n",
    "# Example of fitting the model after confirming shapes\n",
    "history = siamea.fit(\n",
    "    [q_train, a_train], \n",
    "    y_train, \n",
    "    epochs=10,  # You can set a higher number of epochs\n",
    "    batch_size=32,\n",
    "    validation_data=([q_val, a_val], y_val),\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Siamea Model, it is obvious that the epoch 1 is all we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_data):\n",
    "    \"\"\"\n",
    "    Calculate prediction accuracy on test data\n",
    "    \"\"\"\n",
    "    q_test, a_test, y_test = test_data\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = model.predict([q_test, a_test])\n",
    "    predictions = (predictions > 0.9).astype(int).flatten() #0.5 was acceptable, but I tuned the probability cut-off to increase the accuracy.\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "evaluate_model(siamea, test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "There is only 54% accuracy on the test dataset. Indicating that the Siamea model with the simple 2 hidden layer architecture was not performing very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_model_and_find_failures(model, test_data):\n",
    "    \"\"\"\n",
    "    Calculate prediction accuracy and identify failure cases\n",
    "    \"\"\"\n",
    "    q_test, a_test, y_test = test_data\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = model.predict([q_test, a_test])\n",
    "    predictions = (predictions > 0.9).astype(int).flatten()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    \n",
    "    # Identify where predictions are incorrect\n",
    "    failures = np.where(predictions != y_test)[0]\n",
    "    \n",
    "    # Print an example of failure\n",
    "    if len(failures) > 0:\n",
    "        idx = failures[0]  # Example of the first failure\n",
    "        print(f\"\\nExample of failure at index {idx}:\")\n",
    "        print(f\"Predicted: {predictions[idx]}, Actual: {y_test[idx]}\")\n",
    "        print(f\"Question: {val_df['qtext'].iloc[idx]}\")\n",
    "        print(f\"Answer: {val_df['atext'].iloc[idx]}\")\n",
    "    else:\n",
    "        print(\"No failures found!\")\n",
    "    \n",
    "    return accuracy, failures\n",
    "\n",
    "# Call the function to evaluate and find failures\n",
    "accuracy, failures = evaluate_model_and_find_failures(siamea, test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q_test, a_test, y_test = test_features\n",
    "\n",
    "# Get predictions\n",
    "predictions = siamea.predict([q_test, a_test])\n",
    "predictions = (predictions > 0.9).astype(int).flatten()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "failures = np.where(predictions != y_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# zConfirming the code is working fine.\n",
    "print(\"Number of failures:\", len(failures))\n",
    "print(\"Number of predictions:\", len(predictions))\n",
    "print(\"Failure indices:\", failures[:10])  # Print first 10 indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for num in failures:\n",
    "    questions.append(test_df['qtext'].iloc[num])\n",
    "    answers.append(test_df['atext'].iloc[num])\n",
    "\n",
    "# Create a DataFrame of failures (unsuccessful q and a matching based on the prediction.)\n",
    "failures_df = pd.DataFrame({\n",
    "    'question': questions,\n",
    "    'answer': answers\n",
    "})\n",
    "\n",
    "print(failures_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Confirming the prediction was wrong for these 3 datapoints. predicted as 0 instead of 1\n",
    "print(predictions)\n",
    "\n",
    "test_df[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could assume the reasons for the low accuracy as well as the error are due to: \n",
    "\n",
    "**Simple Model:** An application of simple model like Siameas model especially with such a simple network architecture (2 x hidden layers.) Instead, we could use BERT which will be discussed in the second task. Siameas Network does not have a better understanding towards words as a whole compared to BERT. This could be a second reason.\n",
    "\n",
    "**Word Understanding:** There is a possibility that this Siameas Network does not understand how combination of words mean together. For example, when we see words \"capital\" \"Japan\", then we can connect these 2 and derive \"Tokyo\", but in this case the Siameas probably just see \"capital\" and match the best answer that has any capital names in to the question. Without understanding the whole meaning.\n",
    "\n",
    "**Insufficient Training Data:** The model possibly hasn’t seen enough examples of non-matching pairs during training, and this could cause a struggle around differentiating between highly dissimilar answers.\n",
    "\n",
    "\n",
    "### Potential Solutions:\n",
    "\n",
    "**Use More Complex Architectures:** We can use models that capture deeper contextual meaning, such as BERT or transformers, which are better at understanding the relationship between words in context. Also, adding drop layers or conv layers might improve the accuracy. 2 x hidden layers is too simple for a neural network model.\n",
    "\n",
    "**Increase Training Data:** Feed the model more examples of difficult non-matching pairs.\n",
    "\n",
    "**Feature Engineering:** Add other types of features like sentence embeddings, in combination with TF-IDF vectors. This will allow the model to better understand semantic relationships between sentences, and help matching the questions to answers that are closely related."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Task 2 (12 marks): Transformer\n",
    "\n",
    "In this task, let's use Transformer to predict whether two sentences are related or not. Implement a simple Transformer neural network that meets the following requirements:\n",
    "\n",
    "1. (1 mark) Each input for this model should be a concatenation of qtext and atext. Use [SEP] to separate qtext and atext, e.g., \"Can high blood pressure bring on heart failure? [SEP] Hypertensive heart disease is the No.\" You need to pad the input to a fixed length. How do you determine a suitable length?\n",
    "2. (1.5 marks) Choose a suitable tokenizer and justify your choice.\n",
    "3. (1 mark) An embedding layer that generates embedding vectors of the sentence text into size 128. Remember to add position embedding.\n",
    "4. (1 mark) One transformer encoder layer, you need to find a hidden dimension in {64, 128, 256}. Use 3 heads in MultiHeadAttention.\n",
    "5. (1 mark) Do we need a transformer decoder layer for this task? If yes, find a hidden dimension in {64, 128, 256} and use 3 heads in MultiHeadAttention. If no, explain why.\n",
    "6. (0.5 marks) 1 hidden layer with size 256 and ReLU activation function.\n",
    "7. (0.5 marks) 1 output layer with size 2 for binary classification to predict whether two inputs are related or not. \n",
    "8. (1 mark) Choose a suitable loss to train the model\n",
    "9. (1 mark) Report your best accuracy on the test split.\n",
    "10. (1.5 marks) Give an example of a failure case, and explain the possible reason and discuss a potential solution.\n",
    "11. (1 mark) Good coding style as explained in the above Assessment Section.\n",
    "12. (1 mark) Correctly feeding data into your model, and correctly training and testing of your models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Padding Length Shall We Use?\n",
    "\n",
    "We can start with a statistical approach to find an appropriate padding length. Even though this is not a defenitive length, this will serve as a good starting point (in our case 50.) We can examine the impact of padding length increase and fine tune it later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "val_df = pd.read_csv('val.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_lengths(df):\n",
    "    # Combine qtext and atext with [SEP] token\n",
    "    qa_texts = df['qtext'] + ' [SEP] ' + df['atext']\n",
    "    \n",
    "    # Count words in each text in combined texts\n",
    "    lengths = [len(text.split()) for text in qa_texts]\n",
    "    \n",
    "    print(f\"Mean length: {np.mean(lengths):.2f}\")\n",
    "    print(f\"Median length: {np.median(lengths):.2f}\")\n",
    "    print(f\"95th percentile: {np.percentile(lengths, 95):.2f}\")\n",
    "    print(f\"Max length: {np.max(lengths)}\")\n",
    "    \n",
    "    # Return 95th percentile as recommended max_length\n",
    "    return int(np.percentile(lengths, 95))\n",
    "\n",
    "max_length = get_lengths(train_df)\n",
    "print(\"---\")\n",
    "get_lengths(val_df)\n",
    "print(\"---\")\n",
    "get_lengths(test_df)\n",
    "\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine texts with [SEP] token and print\n",
    "qa_texts = train_df['qtext'] + ' [SEP] ' + train_df['atext']\n",
    "qa_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize and fit tokenizer with max features = 5000\n",
    "max_features = 10000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(qa_texts)\n",
    "\n",
    "# Checking if the tokenizing was successfull\n",
    "print(\"Word index sample:\", list(tokenizer.word_index.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transform texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(qa_texts)\n",
    "\n",
    "# Pad sequences to the determined max length\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Checking the shape of the padded sequences\n",
    "print(\"Shape of padded sequences:\", padded_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fix for the embedding layer creation\n",
    "def create_embedding_layer(vocab_size, max_length, embed_dim=128):\n",
    "    input_seq = layers.Input(shape=(max_length,))\n",
    "    \n",
    "    # Word embedding\n",
    "    embedding_layer = layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embed_dim,\n",
    "        input_length=max_length\n",
    "    )\n",
    "    \n",
    "    # Get word embeddings\n",
    "    word_embeddings = embedding_layer(input_seq)\n",
    "    \n",
    "    # Create positional encodings\n",
    "    positions = tf.range(start=0, limit=max_length, delta=1, dtype=tf.float32)\n",
    "    positions = tf.expand_dims(positions, axis=1)\n",
    "    \n",
    "    # Calculate angles for positional encoding\n",
    "    angles = tf.range(start=0, limit=embed_dim, delta=2, dtype=tf.float32)\n",
    "    angles = angles * (-np.log(5000.0) / embed_dim)\n",
    "    angles = tf.expand_dims(angles, axis=0)\n",
    "    \n",
    "    # Calculate positional encodings\n",
    "    pos_encoding = positions * tf.exp(angles)\n",
    "    pos_encoding = tf.concat(\n",
    "        [tf.sin(pos_encoding), tf.cos(pos_encoding)],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Add batch dimension\n",
    "    pos_encoding = tf.expand_dims(pos_encoding, axis=0)\n",
    "    \n",
    "    # Add positional encoding to word embeddings\n",
    "    final_embeddings = word_embeddings + tf.cast(pos_encoding, dtype=tf.float32)\n",
    "    \n",
    "    return tf.keras.Model(inputs=input_seq, outputs=final_embeddings)\n",
    "\n",
    "# Create the complete model\n",
    "def create_complete_model(vocab_size, max_length=50, embed_dim=128, hidden_dim=256, num_heads=3):\n",
    "    # Input layer\n",
    "    input_seq = layers.Input(shape=(max_length,))\n",
    "    \n",
    "    # Embedding layer with positional encoding\n",
    "    embedding_layer = create_embedding_layer(vocab_size, max_length, embed_dim)\n",
    "    embeddings = embedding_layer(input_seq)\n",
    "    \n",
    "    # Transformer encoder\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=hidden_dim\n",
    "    )(embeddings, embeddings)\n",
    "    attention_output = layers.LayerNormalization(epsilon=1e-6)(attention_output + embeddings)\n",
    "    \n",
    "    # Feed-forward network\n",
    "    ff_output = layers.Dense(hidden_dim, activation=\"relu\")(attention_output)\n",
    "    ff_output = layers.Dense(embed_dim)(ff_output)\n",
    "    encoder_output = layers.LayerNormalization(epsilon=1e-6)(ff_output + attention_output)\n",
    "    \n",
    "    # Global average pooling\n",
    "    pooled_output = layers.GlobalAveragePooling1D()(encoder_output)\n",
    "\n",
    "    hidden_output = layers.Dense(256, activation=\"relu\")(pooled_output)\n",
    "    \n",
    "    # Output layer - accepting binary output hence 2 not 1\n",
    "    output = layers.Dense(2, activation=\"softmax\")(hidden_output)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=input_seq, outputs=output)\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create a model with MultiHead neuron size 256\n",
    "model1 = create_complete_model(\n",
    "    vocab_size=max_features,\n",
    "    max_length=max_length,\n",
    "    embed_dim=128,\n",
    "    hidden_dim=256,\n",
    "    num_heads=3\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "train_qa_texts = train_df['qtext'] + ' [SEP] ' + train_df['atext']\n",
    "train_sequences = tokenizer.texts_to_sequences(train_qa_texts)\n",
    "train_sequences = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
    "train_labels = train_df['label']  # Assuming label column is named 'label'\n",
    "\n",
    "# Prepare validation data\n",
    "val_qa_texts = val_df['qtext'] + ' [SEP] ' + val_df['atext']\n",
    "val_sequences = tokenizer.texts_to_sequences(val_qa_texts)\n",
    "val_sequences = pad_sequences(val_sequences, maxlen=max_length, padding='post')\n",
    "val_labels = val_df['label']\n",
    "\n",
    "# Prepare test data\n",
    "test_qa_texts = test_df['qtext'] + ' [SEP] ' + test_df['atext']\n",
    "test_sequences = tokenizer.texts_to_sequences(test_qa_texts)\n",
    "test_sequences = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# As we are ready to feed the prepared data, we will train model defined earlier here.\n",
    "history = model1.fit(\n",
    "    train_sequences,\n",
    "    train_labels,\n",
    "    epochs=10, \n",
    "    batch_size=32, \n",
    "    validation_data=(val_sequences, val_labels),\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Thanks to the early stopping, we didn't need to go through 10 total epochs (it would take much longer..)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model1.evaluate(test_sequences, test_labels)\n",
    "print(f\"Test accuracy with 256 multihead: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a model with MultiHead neuron size 256\n",
    "model2 = create_complete_model(\n",
    "    vocab_size=max_features,\n",
    "    max_length=max_length,\n",
    "    embed_dim=128,\n",
    "    hidden_dim=128,\n",
    "    num_heads=3\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "model2.summary()\n",
    "\n",
    "history = model2.fit(\n",
    "    train_sequences,\n",
    "    train_labels,\n",
    "    epochs=10, \n",
    "    batch_size=32, \n",
    "    validation_data=(val_sequences, val_labels),\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_loss, test_accuracy = model2.evaluate(test_sequences, test_labels)\n",
    "print(f\"Test accuracy with 128 multihead: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a model with MultiHead neuron size 256\n",
    "model3 = create_complete_model(\n",
    "    vocab_size=max_features,\n",
    "    max_length=max_length,\n",
    "    embed_dim=128,\n",
    "    hidden_dim=64,\n",
    "    num_heads=3\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "model3.summary()\n",
    "\n",
    "history = model3.fit(\n",
    "    train_sequences,\n",
    "    train_labels,\n",
    "    epochs=10, \n",
    "    batch_size=32, \n",
    "    validation_data=(val_sequences, val_labels),\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_loss, test_accuracy = model3.evaluate(test_sequences, test_labels)\n",
    "print(f\"Test accuracy with 64 multihead: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this task is binary classification, we don't need a transformer decoder layer. The encoder layer is enough to capture relationships between tokens in each input pair. The decoder is generally used in tasks that involve sequence generation, such as translation.\n",
    "\n",
    "As reported above, test accuracies based on the neuron number in MultiHead Layer are below:\n",
    "\n",
    "- Model1 (256) : 0.538731038570404\n",
    "- Model2 (128) : 0.49785909056663513\n",
    "- Model3 (64) : 0.5762943029403687\n",
    "\n",
    "And we can see that the Model3 shows slight improvement compared to the Siameas model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get predictions on the test set\n",
    "test_predictions = model3.predict(test_sequences)\n",
    "test_predicted_labels = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "# Compare predicted labels with true labels and identify misclassified examples\n",
    "misclassified_indices = np.where(test_predicted_labels != test_labels)[0]\n",
    "\n",
    "# Print first 10 misclassified question-answer pairs\n",
    "print(\"\\nMisclassified Examples:\")\n",
    "for i in misclassified_indices[:10]:\n",
    "    print(f\"Actual label: {test_labels[i]}, Predicted label: {test_predicted_labels[i]}\")\n",
    "    print(f\"Question: {test_df['qtext'].iloc[i]}\")\n",
    "    print(f\"Answer: {test_df['atext'].iloc[i]}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "We could assume the reasons for the low accuracy as well as the error are due to: \n",
    "\n",
    "**Low Contextual Understanding:** The model seems to struggle when the answer contains information that is not directly relevant to the question. For example, in the case of \"What are some symptoms of an insulin overdose?\", the answer talks about hypoglycemia, which is related to low blood sugar, not an insulin overdose.\n",
    "\n",
    "**Insufficient Training Data:** The model possibly hasn’t seen enough examples of non-matching pairs during training, and this could cause a struggle around differentiating between highly dissimilar answers.\n",
    "\n",
    "### Potential Solutions:\n",
    "\n",
    "**Try Advanced Language Models and Compare the Performance:** we can try to see improvement of contextual understanding bt applying additional context features like entity extraction, semantic OR applying more advanced language models like BERT, GPT that can better capture the semantic relationships.\n",
    "\n",
    "**Increase Training Data:** Feed the model more examples of difficult non-matching pairs."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "A2_solution.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "a7b63e7410c98f344f02082f10d790581d1dba1eeb1c8fe30f342f6109f0429e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
